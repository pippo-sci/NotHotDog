{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "55029c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a721e227",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Loading labels started...\n",
      "INFO:root:Dataset labels finished\n",
      "INFO:root:Loading Images from Dataset/open-images-v7/train started...\n",
      "INFO:root:1440 Images loaded\n",
      "INFO:root:Preprocess one started...\n",
      "INFO:root:Preprocess one finish\n",
      "INFO:root:Data Augmentation started...\n",
      "INFO:root:Data Augmentation finished\n"
     ]
    }
   ],
   "source": [
    "import processing_pipeline as pp\n",
    "\n",
    "path = \"Dataset/open-images-v7/train\"\n",
    "\n",
    "current_process = pp.preprocess()\n",
    "current_process.set_dataframe(path)\n",
    "current_process.load_data_img(path)\n",
    "current_process.process_one()\n",
    "df = current_process.augmentation_process()\n",
    "#current_process.out_put(\"/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "60fbf290",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get dataset and label hotdogs\n",
    "df = current_process.rebuild_dataset()\n",
    "df['n_label'] = df.label.apply(lambda x: 1 if x==\"Hot dog\" else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a5d68580",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(df, fraction=0.5):\n",
    "  # Assuming you have a df\n",
    "  # Randomly sample fraction% of the DataFrame rows without replacement\n",
    "  sampled_df = df.sample(frac=fraction, replace=False)\n",
    "  return sampled_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9b9cc485",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hog_features(X_train, X_test):\n",
    "    import numpy as np\n",
    "    from skimage.feature import hog\n",
    "\n",
    "    # Assume you have X train, y train with image\n",
    "    # Extract HOG features for each image\n",
    "    hog_features_train = []\n",
    "    hog_features_test = []\n",
    "\n",
    "    for image in X_train:\n",
    "        hog_features = hog(image, orientations=9, pixels_per_cell=(16, 16), cells_per_block=(2, 2), visualize=False)\n",
    "        hog_features_train.append(hog_features)\n",
    "\n",
    "    for image in X_test:\n",
    "        hog_features = hog(image, orientations=9, pixels_per_cell=(16, 16), cells_per_block=(2, 2), visualize=False)\n",
    "        hog_features_test.append(hog_features)\n",
    "\n",
    "    # Convert the features lists to numpy arrays\n",
    "    X_train_hog = np.array(hog_features_train)\n",
    "    X_test_hog = np.array(hog_features_test)\n",
    "    \n",
    "    return X_train_hog, X_test_hog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2a3900b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_raw_pixel_values(X_train, X_test):\n",
    "  # Raw pixel values\n",
    "  # You can flatten the image arrays and use the raw pixel values as features. \n",
    "  # Each pixel value will be considered as a separate feature. \n",
    "  # This approach captures the raw intensity values of each pixel in the image.\n",
    "  # Assuming your image data is stored in a DataFrame called 'df' with a column named 'image'\n",
    "  # Convert the image arrays to a numpy array\n",
    "  # Reshape the images to a 2D array (num_samples, num_pixels)\n",
    "  num_samples = X_train.shape[0]\n",
    "  raw_values_train = images.reshape(num_samples, -1)\n",
    "  \n",
    "  num_samples = X_test.shape[0]\n",
    "  raw_values_test = images.reshape(num_samples, -1)\n",
    "  \n",
    "  return raw_values_train, raw_values_test\n",
    "\n",
    "def get_histograms(X_train, X_test):\n",
    "  import numpy as np\n",
    "  #Histogram of Pixel Intensities:\n",
    "  #You can calculate histograms of pixel intensities to capture the distribution of intensity values across the image. \n",
    "  # This approach provides information about the overall brightness and contrast of the image.\n",
    "  # Calculate histograms for each image\n",
    "  histograms = np.array([np.histogram(image, bins=256, range=(0, 256))[0] for image in X_train])\n",
    "  # Normalize the histograms\n",
    "  histogram_train = histograms / histograms.sum(axis=1, keepdims=True)\n",
    "  \n",
    "  histograms = np.array([np.histogram(image, bins=256, range=(0, 256))[0] for image in X_test])\n",
    "  # Normalize the histograms\n",
    "  histogram_test = histograms / histograms.sum(axis=1, keepdims=True)\n",
    "  \n",
    "  return histogram_train, histogram_test\n",
    "\n",
    "def get_canny_edges(X_train, X_test):\n",
    "  import cv2\n",
    "  import numpy as np\n",
    "  import pandas as pd\n",
    "  # Apply Canny edge detection to each image\n",
    "  edges = np.array([cv2.Canny((image*255).astype(np.uint8), threshold1=100, threshold2=200) for image in X_train])\n",
    "  # Reshape the edge arrays to a 2D array (num_samples, num_pixels)\n",
    "  num_samples = X_train.shape[0]\n",
    "  canny_edge_train = edges.reshape(num_samples, -1)\n",
    "  \n",
    "  edges = np.array([cv2.Canny((image*255).astype(np.uint8), threshold1=100, threshold2=200) for image in X_test])\n",
    "  num_samples = X_test.shape[0]\n",
    "  canny_edge_test = edges.reshape(num_samples, -1)\n",
    "  \n",
    "  return canny_edge_train, canny_edge_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "271006bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_images(X_train, X_test):\n",
    "  import numpy as np\n",
    "  from skimage.feature import hog\n",
    "  from sklearn.preprocessing import StandardScaler\n",
    "  import pandas as pd\n",
    "\n",
    "  print('Starting preprocess...')\n",
    "  print('Getting HOG features...')\n",
    "  X_train_hog, X_test_hog = hog_features(X_train, X_test)\n",
    "  print('Getting Histograms...')\n",
    "  X_train_histogram, X_test_histogram = get_histograms(X_train, X_test)\n",
    "  \n",
    "  X_train_expanded = pd.concat([pd.DataFrame(X_train_hog), pd.DataFrame(X_train_histogram)], axis=1)\n",
    "  X_test_expanded = pd.concat([pd.DataFrame(X_test_hog), pd.DataFrame(X_test_histogram)], axis=1)\n",
    "  \n",
    "  print('Merging & scaling data...')\n",
    "  # Scale the features\n",
    "  scaler = StandardScaler()\n",
    "  X_train_scaled = scaler.fit_transform(X_train_expanded)\n",
    "  X_test_scaled = scaler.transform(X_test_expanded)\n",
    "  \n",
    "  return X_train_scaled, X_test_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aa7e528e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_dimension_pca(X_train, X_test, n_components):\n",
    "    import pandas as pd\n",
    "    from sklearn.decomposition import PCA\n",
    "\n",
    "    print(\"Performing PCA...\")\n",
    "    # Perform PCA on training data\n",
    "    pca = PCA(n_components=n_components)\n",
    "    X_train_pca = pca.fit_transform(X_train)\n",
    "    \n",
    "    # Calculate percentage of variation explained\n",
    "    explained_variance_ratio = pca.explained_variance_ratio_\n",
    "    total_variance_explained = sum(explained_variance_ratio[:n_components]) * 100\n",
    "    print(\"Total variance explaned: {}\".format(total_variance_explained))\n",
    "    \n",
    "    print(\"Applying PCA...\")\n",
    "    # Apply PCA transformation to test data\n",
    "    X_test_pca = pca.transform(X_test)\n",
    "    \n",
    "    # Create data frames from reduced dimension data\n",
    "    X_train_pca_df = pd.DataFrame(X_train_pca, columns=[f'PC{i+1}' for i in range(n_components)])\n",
    "    X_test_pca_df = pd.DataFrame(X_test_pca, columns=[f'PC{i+1}' for i in range(n_components)])\n",
    "    \n",
    "    return X_train_pca_df, X_test_pca_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2d24112b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(X, y, model):\n",
    "    import numpy as np\n",
    "    from skimage.feature import hog\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    from sklearn.svm import SVC\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    import pandas as pd\n",
    "    import time\n",
    "\n",
    "    # Assuming your image data is stored in a DataFrame called 'df' with a column named 'image'\n",
    "    # Convert the image arrays to a numpy array\n",
    "    images = np.array(X.tolist())\n",
    "    labels = np.array(y)\n",
    "\n",
    "    # Split the data into training and testing sets\n",
    "    print('Spliting train/test...')\n",
    "    X_train, X_test, y_train, y_test = train_test_split(images, labels, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Apply preprocess\n",
    "    X_train_scaled, X_test_scaled = preprocess_images(X_train, X_test)\n",
    "    \n",
    "    # Reduce dimensionality\n",
    "    n_components = 10\n",
    "    X_train_pca_df, X_test_pca_df = reduce_dimension_pca(X_train_scaled, X_test_scaled, n_components)\n",
    "\n",
    "    print('Training model...')\n",
    "    # Train the model\n",
    "    classifier = model\n",
    "    classifier.fit(X_train_pca_df, y_train)\n",
    "    y_pred_train = classifier.predict(X_train_pca_df)\n",
    "\n",
    "    print('Making predictions...')\n",
    "    # Make predictions on the test set\n",
    "    y_pred = classifier.predict(X_test_pca_df)\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy_train = accuracy_score(y_train, y_pred_train)\n",
    "    accuracy_test = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    print(\"Train Accuracy: {}\".format(accuracy_train))\n",
    "    print(\"Test Accuracy: {}\".format(accuracy_test))\n",
    "    \n",
    "    return y_pred_train, y_pred, accuracy_train, accuracy_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9c1466c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spliting train/test...\n",
      "Starting preprocess...\n",
      "Getting HOG features...\n",
      "Getting Histograms...\n",
      "Merging & scaling data...\n",
      "Performing PCA...\n",
      "Total variance explaned: 12.316759174607089\n",
      "Applying PCA...\n",
      "Training model...\n",
      "Making predictions...\n",
      "Train Accuracy: 1.0\n",
      "Test Accuracy: 0.5986238532110092\n",
      "Total running time in minutes: 4.230105817317963\n"
     ]
    }
   ],
   "source": [
    "# Test with all preprocess stages\n",
    "# Get X, y\n",
    "sampled_df = sample(df, 1)\n",
    "\n",
    "X = sampled_df.img\n",
    "y = sampled_df.n_label\n",
    "\n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# Create model\n",
    "# Stacking \n",
    "from mlxtend.classifier import StackingClassifier\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression, Ridge\n",
    "from sklearn.metrics import f1_score, accuracy_score,classification_report,confusion_matrix\n",
    "from sklearn import metrics\n",
    "\n",
    "# Define regularization parameters\n",
    "regularization_params = {\n",
    "    'logisticregression__C': 50,  # Specify the regularization strength for LogisticRegression\n",
    "    'ridge__alpha': 0.5  # Specify the regularization strength for Ridge\n",
    "}\n",
    "\n",
    "m = StackingClassifier(\n",
    "    classifiers=[\n",
    "        LogisticRegression(C=regularization_params['logisticregression__C']),\n",
    "        KNeighborsRegressor(n_neighbors=2),\n",
    "        DecisionTreeClassifier(random_state=0)\n",
    "    ],\n",
    "    use_probas=False,\n",
    "    meta_classifier=LogisticRegression(C=regularization_params['logisticregression__C'])\n",
    ")\n",
    "\n",
    "y_pred_train, y_pred, accuracy_train, accuracy_test = get_predictions(X, y, m)\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(\"Total running time in minutes: {}\".format((time.time() - start)/60))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "22f6b6c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spliting train/test...\n",
      "Starting preprocess...\n",
      "Getting HOG features...\n",
      "Getting Histograms...\n",
      "Getting Canny Edges...\n",
      "Merging & scaling data...\n",
      "Performing PCA...\n",
      "Total variance explaned: 100.00000000000009\n",
      "Applying PCA...\n",
      "Training model...\n",
      "Making predictions...\n",
      "Train Accuracy: 0.710919540229885\n",
      "Test Accuracy: 0.5229357798165137\n",
      "Total running time in minutes: 4.705825118223826\n"
     ]
    }
   ],
   "source": [
    "# Test with all preprocess stages\n",
    "# Get X, y\n",
    "sampled_df = sample(df, 1)\n",
    "\n",
    "X = sampled_df.img\n",
    "y = sampled_df.n_label\n",
    "\n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# Create model\n",
    "# Stacking \n",
    "from mlxtend.classifier import StackingClassifier\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score, accuracy_score,classification_report,confusion_matrix\n",
    "import xgboost as xgb\n",
    "from sklearn import metrics\n",
    "\n",
    "m = xgb.XGBClassifier()\n",
    "\n",
    "y_pred_train, y_pred, accuracy_train, accuracy_test = get_predictions(X, y, m)\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(\"Total running time in minutes: {}\".format((time.time() - start)/60))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "61039082",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spliting train/test...\n",
      "Starting preprocess...\n",
      "Getting HOG features...\n",
      "Merging & scaling data...\n",
      "Training model...\n",
      "Making predictions...\n",
      "Total running time in minutes: 2.1730300148328143\n",
      "Train Accuracy: 1.0\n",
      "Train Accuracy: 0.5688073394495413\n"
     ]
    }
   ],
   "source": [
    "# Test with just HOG Process\n",
    "# Get X, y\n",
    "sampled_df = sample(df, 0.5)\n",
    "\n",
    "X = sampled_df.img\n",
    "y = sampled_df.n_label\n",
    "\n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# Create model\n",
    "# Stacking \n",
    "from mlxtend.classifier import StackingClassifier\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score, accuracy_score,classification_report,confusion_matrix\n",
    "from sklearn import metrics\n",
    "\n",
    "m = StackingClassifier(\n",
    "    classifiers=[\n",
    "        LogisticRegression(),\n",
    "        KNeighborsRegressor(n_neighbors=2),\n",
    "        DecisionTreeClassifier(random_state=0)\n",
    "    ],\n",
    "    use_probas=False,\n",
    "    meta_classifier=LogisticRegression()\n",
    ")\n",
    "\n",
    "y_pred_train, y_pred, accuracy_train, accuracy_test = get_predictions(X, y, m)\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(\"Total running time in minutes: {}\".format((time.time() - start)/60))\n",
    "print(\"Train Accuracy: {}\".format(accuracy_train))\n",
    "print(\"Train Accuracy: {}\".format(accuracy_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1b3e73be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spliting train/test...\n",
      "Starting preprocess...\n",
      "Getting Canny Edges...\n",
      "Merging & scaling data...\n",
      "Training model...\n",
      "Making predictions...\n",
      "Total running time in minutes: 2.5255083163579304\n",
      "Train Accuracy: 1.0\n",
      "Train Accuracy: 0.4954128440366973\n"
     ]
    }
   ],
   "source": [
    "# Test with just Canny Edges\n",
    "# Get X, y\n",
    "sampled_df = sample(df, 0.5)\n",
    "\n",
    "X = sampled_df.img\n",
    "y = sampled_df.n_label\n",
    "\n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# Create model\n",
    "# Stacking \n",
    "from mlxtend.classifier import StackingClassifier\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score, accuracy_score,classification_report,confusion_matrix\n",
    "from sklearn import metrics\n",
    "\n",
    "m = StackingClassifier(\n",
    "    classifiers=[\n",
    "        LogisticRegression(),\n",
    "        KNeighborsRegressor(n_neighbors=2),\n",
    "        DecisionTreeClassifier(random_state=0)\n",
    "    ],\n",
    "    use_probas=False,\n",
    "    meta_classifier=LogisticRegression()\n",
    ")\n",
    "\n",
    "y_pred_train, y_pred, accuracy_train, accuracy_test = get_predictions(X, y, m)\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(\"Total running time in minutes: {}\".format((time.time() - start)/60))\n",
    "print(\"Train Accuracy: {}\".format(accuracy_train))\n",
    "print(\"Train Accuracy: {}\".format(accuracy_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4f6da9c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spliting train/test...\n",
      "Starting preprocess...\n",
      "Getting Histograms...\n",
      "Merging & scaling data...\n",
      "Training model...\n",
      "Making predictions...\n",
      "Total running time in minutes: 0.8873202999432882\n",
      "Train Accuracy: 0.7241379310344828\n",
      "Train Accuracy: 0.591743119266055\n"
     ]
    }
   ],
   "source": [
    "# Test with just Histograms\n",
    "# Get X, y\n",
    "sampled_df = sample(df, 0.5)\n",
    "\n",
    "X = sampled_df.img\n",
    "y = sampled_df.n_label\n",
    "\n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# Create model\n",
    "# Stacking \n",
    "from mlxtend.classifier import StackingClassifier\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score, accuracy_score,classification_report,confusion_matrix\n",
    "from sklearn import metrics\n",
    "\n",
    "m = StackingClassifier(\n",
    "    classifiers=[\n",
    "        LogisticRegression(),\n",
    "        KNeighborsRegressor(n_neighbors=2),\n",
    "        DecisionTreeClassifier(random_state=0)\n",
    "    ],\n",
    "    use_probas=False,\n",
    "    meta_classifier=LogisticRegression()\n",
    ")\n",
    "\n",
    "y_pred_train, y_pred, accuracy_train, accuracy_test = get_predictions(X, y, m)\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(\"Total running time in minutes: {}\".format((time.time() - start)/60))\n",
    "print(\"Train Accuracy: {}\".format(accuracy_train))\n",
    "print(\"Train Accuracy: {}\".format(accuracy_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "55c3b7b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Convert each element in the Pandas Series to a NumPy array\n",
    "arrays = [np.array(x) for x in X_train]\n",
    "# Stack the NumPy arrays together to create a NumPy array with consistent shapes\n",
    "array = np.stack(arrays)\n",
    "# Convert the NumPy array to a PyTorch tensor\n",
    "tensor_X_train = torch.from_numpy(array)\n",
    "\n",
    "# Convert each element in the Pandas Series to a NumPy array\n",
    "arrays = [np.array(x) for x in y_train]\n",
    "# Stack the NumPy arrays together to create a NumPy array with consistent shapes\n",
    "array = np.stack(arrays)\n",
    "# Convert the NumPy array to a PyTorch tensor\n",
    "tensor_y_train = torch.from_numpy(array)\n",
    "\n",
    "# Convert each element in the Pandas Series to a NumPy array\n",
    "arrays = [np.array(x) for x in X_test]\n",
    "# Stack the NumPy arrays together to create a NumPy array with consistent shapes\n",
    "array = np.stack(arrays)\n",
    "# Convert the NumPy array to a PyTorch tensor\n",
    "tensor_X_test = torch.from_numpy(array)\n",
    "\n",
    "# Convert each element in the Pandas Series to a NumPy array\n",
    "arrays = [np.array(x) for x in y_test]\n",
    "# Stack the NumPy arrays together to create a NumPy array with consistent shapes\n",
    "array = np.stack(arrays)\n",
    "# Convert the NumPy array to a PyTorch tensor\n",
    "tensor_y_test = torch.from_numpy(array)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f161809e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Loss: 49.20992986505682\n",
      "Epoch 2 - Loss: 49.84848487160423\n",
      "Epoch 3 - Loss: 50.132575711337005\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCannot execute code, session has been disposed. Please try restarting the Kernel."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "img_size = 600\n",
    "\n",
    "# Define the CNN model architecture\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)  # Update input channels to 1\n",
    "        self.relu = nn.ReLU()\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.fc1 = nn.Linear(64 * (img_size // 4) * (img_size // 4), 128)\n",
    "        self.fc2 = nn.Linear(128, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "# Create an instance of the CNN model\n",
    "model = CNN()\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Convert your dataset to PyTorch tensors and dataloaders\n",
    "# Assuming you have preprocessed tensors X_train, y_train, X_test, y_test\n",
    "\n",
    "train_dataset = torch.utils.data.TensorDataset(tensor_X_train.float(), tensor_y_train)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "test_dataset = torch.utils.data.TensorDataset(tensor_X_test.float(), tensor_y_test)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(10):\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs = inputs.unsqueeze(1)  # Add extra dimension for grayscale channel\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels.unsqueeze(1).float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    print(f\"Epoch {epoch + 1} - Loss: {running_loss / len(train_loader)}\")\n",
    "\n",
    "# Evaluate the model\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs = inputs.unsqueeze(1)  # Add extra dimension for grayscale channel\n",
    "        outputs = model(inputs)\n",
    "        predicted = (outputs >= 0.5).squeeze().long()\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "accuracy = correct / total\n",
    "print(f\"Test Accuracy: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f7c36ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "vscode": {
   "interpreter": {
    "hash": "18908a4771d6709b9c7e8530bc65fed391702e30944b52b387d089dd2ff29e9d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
